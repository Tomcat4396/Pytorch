{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "#导入pytorch一个完整流程所需的可能全部的包\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models as m\n",
    "from torch.utils.data import DataLoader\n",
    "torch.backends.cudnn.benchmark=True #用于加速GPU运算的代码\n",
    "#导入作为辅助工具的各类包\n",
    "import matplotlib.pyplot as plt #可视化\n",
    "from time import time #计算时间、记录时间\n",
    "import datetime\n",
    "import random #控制随机性\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc #garbage collector 垃圾回收\n",
    "# from plotpic import plotsample\n",
    "from torchinfo import summary\n",
    "# from EarlyStop import EarlyStopping\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1412) #random\n",
    "np.random.seed(1412) #numpy.random\n",
    "torch.cuda.manual_seed(1412)\n",
    "torch.cuda.manual_seed_all(1412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据增强\n",
    "trainT = T.Compose([T.RandomResizedCrop(224)\n",
    "                   ,T.RandomRotation(degrees=[-5, 5])\n",
    "                   # ,T.RandomHorizontalFlip()\n",
    "                   ,T.ToTensor()\n",
    "                   ,T.Normalize(mean = [0.485, 0.456, 0.406]\n",
    "                                ,std = [0.229, 0.224, 0.225])\n",
    "                     ])\n",
    "testT = T.Compose([\n",
    "                    T.CenterCrop(224)\n",
    "                      ,T.ToTensor()\n",
    "                      ,T.Normalize(mean = [0.485, 0.456, 0.406]\n",
    "                                   ,std = [0.229, 0.224, 0.225])\n",
    "                   ]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.ImageFolder(root=r\"F:\\Data\\New\\train\",\n",
    "                                                 transform = trainT\n",
    "                                                 )\n",
    "test = torchvision.datasets.ImageFolder(root=r\"F:\\Data\\New\\test\",\n",
    "                                         transform = testT\n",
    "                                         )\n",
    "\n",
    "# num_samples = len(train)\n",
    "# print(f\"Number of samples in train dataset: {num_samples}\")\n",
    "# train = torchvision.datasets.SVHN(root = 'D://onedrive//OneDrive - yourdrive//Code//Notebook_workspace//SLPython//05Pytorch//Lesson 17//SVHN'\n",
    "#                                   ,split = \"train\"\n",
    "#                                   ,download = False\n",
    "#                                   ,transform = trainT\n",
    "#                                   )\n",
    "# test = torchvision.datasets.SVHN(root = 'D://onedrive//OneDrive - yourdrive//Code//Notebook_workspace//SLPython//05Pytorch//Lesson 17//SVHN'\n",
    "#                                  ,split = 'test'\n",
    "#                                  ,download = False\n",
    "#                                  ,transform = testT\n",
    "#                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# 获取一个样本（图像和标签）\n",
    "sample_index = 500 # 可以选择任意索引\n",
    "img, label = train[sample_index]\n",
    "\n",
    "# 将张量图像转换回PIL图像\n",
    "unnormalized_img = T.ToPILImage()(img)\n",
    "\n",
    "# 显示图像和标签\n",
    "print(f\"Label: {label}\")\n",
    "unnormalized_img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (6): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (7): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (8): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (9): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (10): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (11): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (12): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (13): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (14): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (15): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (16): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (17): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (18): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (19): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (20): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (21): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (22): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMyResNet50                               [10, 11]                  --\n├─Conv2d: 1-1                            [10, 64, 11, 11]          9,408\n├─BatchNorm2d: 1-2                       [10, 64, 11, 11]          128\n├─ReLU: 1-3                              [10, 64, 11, 11]          --\n├─MaxPool2d: 1-4                         [10, 64, 6, 6]            --\n├─Sequential: 1-5                        [10, 256, 6, 6]           --\n│    └─Bottleneck: 2-1                   [10, 256, 6, 6]           --\n│    │    └─Conv2d: 3-1                  [10, 64, 6, 6]            4,096\n│    │    └─BatchNorm2d: 3-2             [10, 64, 6, 6]            128\n│    │    └─ReLU: 3-3                    [10, 64, 6, 6]            --\n│    │    └─Conv2d: 3-4                  [10, 64, 6, 6]            36,864\n│    │    └─BatchNorm2d: 3-5             [10, 64, 6, 6]            128\n│    │    └─ReLU: 3-6                    [10, 64, 6, 6]            --\n│    │    └─Conv2d: 3-7                  [10, 256, 6, 6]           16,384\n│    │    └─BatchNorm2d: 3-8             [10, 256, 6, 6]           512\n│    │    └─Sequential: 3-9              [10, 256, 6, 6]           16,896\n│    │    └─ReLU: 3-10                   [10, 256, 6, 6]           --\n│    └─Bottleneck: 2-2                   [10, 256, 6, 6]           --\n│    │    └─Conv2d: 3-11                 [10, 64, 6, 6]            16,384\n│    │    └─BatchNorm2d: 3-12            [10, 64, 6, 6]            128\n│    │    └─ReLU: 3-13                   [10, 64, 6, 6]            --\n│    │    └─Conv2d: 3-14                 [10, 64, 6, 6]            36,864\n│    │    └─BatchNorm2d: 3-15            [10, 64, 6, 6]            128\n│    │    └─ReLU: 3-16                   [10, 64, 6, 6]            --\n│    │    └─Conv2d: 3-17                 [10, 256, 6, 6]           16,384\n│    │    └─BatchNorm2d: 3-18            [10, 256, 6, 6]           512\n│    │    └─ReLU: 3-19                   [10, 256, 6, 6]           --\n│    └─Bottleneck: 2-3                   [10, 256, 6, 6]           --\n│    │    └─Conv2d: 3-20                 [10, 64, 6, 6]            16,384\n│    │    └─BatchNorm2d: 3-21            [10, 64, 6, 6]            128\n│    │    └─ReLU: 3-22                   [10, 64, 6, 6]            --\n│    │    └─Conv2d: 3-23                 [10, 64, 6, 6]            36,864\n│    │    └─BatchNorm2d: 3-24            [10, 64, 6, 6]            128\n│    │    └─ReLU: 3-25                   [10, 64, 6, 6]            --\n│    │    └─Conv2d: 3-26                 [10, 256, 6, 6]           16,384\n│    │    └─BatchNorm2d: 3-27            [10, 256, 6, 6]           512\n│    │    └─ReLU: 3-28                   [10, 256, 6, 6]           --\n├─Sequential: 1-6                        [10, 512, 3, 3]           --\n│    └─Bottleneck: 2-4                   [10, 512, 3, 3]           --\n│    │    └─Conv2d: 3-29                 [10, 128, 6, 6]           32,768\n│    │    └─BatchNorm2d: 3-30            [10, 128, 6, 6]           256\n│    │    └─ReLU: 3-31                   [10, 128, 6, 6]           --\n│    │    └─Conv2d: 3-32                 [10, 128, 3, 3]           147,456\n│    │    └─BatchNorm2d: 3-33            [10, 128, 3, 3]           256\n│    │    └─ReLU: 3-34                   [10, 128, 3, 3]           --\n│    │    └─Conv2d: 3-35                 [10, 512, 3, 3]           65,536\n│    │    └─BatchNorm2d: 3-36            [10, 512, 3, 3]           1,024\n│    │    └─Sequential: 3-37             [10, 512, 3, 3]           132,096\n│    │    └─ReLU: 3-38                   [10, 512, 3, 3]           --\n│    └─Bottleneck: 2-5                   [10, 512, 3, 3]           --\n│    │    └─Conv2d: 3-39                 [10, 128, 3, 3]           65,536\n│    │    └─BatchNorm2d: 3-40            [10, 128, 3, 3]           256\n│    │    └─ReLU: 3-41                   [10, 128, 3, 3]           --\n│    │    └─Conv2d: 3-42                 [10, 128, 3, 3]           147,456\n│    │    └─BatchNorm2d: 3-43            [10, 128, 3, 3]           256\n│    │    └─ReLU: 3-44                   [10, 128, 3, 3]           --\n│    │    └─Conv2d: 3-45                 [10, 512, 3, 3]           65,536\n│    │    └─BatchNorm2d: 3-46            [10, 512, 3, 3]           1,024\n│    │    └─ReLU: 3-47                   [10, 512, 3, 3]           --\n│    └─Bottleneck: 2-6                   [10, 512, 3, 3]           --\n│    │    └─Conv2d: 3-48                 [10, 128, 3, 3]           65,536\n│    │    └─BatchNorm2d: 3-49            [10, 128, 3, 3]           256\n│    │    └─ReLU: 3-50                   [10, 128, 3, 3]           --\n│    │    └─Conv2d: 3-51                 [10, 128, 3, 3]           147,456\n│    │    └─BatchNorm2d: 3-52            [10, 128, 3, 3]           256\n│    │    └─ReLU: 3-53                   [10, 128, 3, 3]           --\n│    │    └─Conv2d: 3-54                 [10, 512, 3, 3]           65,536\n│    │    └─BatchNorm2d: 3-55            [10, 512, 3, 3]           1,024\n│    │    └─ReLU: 3-56                   [10, 512, 3, 3]           --\n│    └─Bottleneck: 2-7                   [10, 512, 3, 3]           --\n│    │    └─Conv2d: 3-57                 [10, 128, 3, 3]           65,536\n│    │    └─BatchNorm2d: 3-58            [10, 128, 3, 3]           256\n│    │    └─ReLU: 3-59                   [10, 128, 3, 3]           --\n│    │    └─Conv2d: 3-60                 [10, 128, 3, 3]           147,456\n│    │    └─BatchNorm2d: 3-61            [10, 128, 3, 3]           256\n│    │    └─ReLU: 3-62                   [10, 128, 3, 3]           --\n│    │    └─Conv2d: 3-63                 [10, 512, 3, 3]           65,536\n│    │    └─BatchNorm2d: 3-64            [10, 512, 3, 3]           1,024\n│    │    └─ReLU: 3-65                   [10, 512, 3, 3]           --\n├─Sequential: 1-7                        [10, 1024, 2, 2]          --\n│    └─Bottleneck: 2-8                   [10, 1024, 2, 2]          --\n│    │    └─Conv2d: 3-66                 [10, 256, 3, 3]           131,072\n│    │    └─BatchNorm2d: 3-67            [10, 256, 3, 3]           512\n│    │    └─ReLU: 3-68                   [10, 256, 3, 3]           --\n│    │    └─Conv2d: 3-69                 [10, 256, 2, 2]           589,824\n│    │    └─BatchNorm2d: 3-70            [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-71                   [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-72                 [10, 1024, 2, 2]          262,144\n│    │    └─BatchNorm2d: 3-73            [10, 1024, 2, 2]          2,048\n│    │    └─Sequential: 3-74             [10, 1024, 2, 2]          526,336\n│    │    └─ReLU: 3-75                   [10, 1024, 2, 2]          --\n│    └─Bottleneck: 2-9                   [10, 1024, 2, 2]          --\n│    │    └─Conv2d: 3-76                 [10, 256, 2, 2]           262,144\n│    │    └─BatchNorm2d: 3-77            [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-78                   [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-79                 [10, 256, 2, 2]           589,824\n│    │    └─BatchNorm2d: 3-80            [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-81                   [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-82                 [10, 1024, 2, 2]          262,144\n│    │    └─BatchNorm2d: 3-83            [10, 1024, 2, 2]          2,048\n│    │    └─ReLU: 3-84                   [10, 1024, 2, 2]          --\n│    └─Bottleneck: 2-10                  [10, 1024, 2, 2]          --\n│    │    └─Conv2d: 3-85                 [10, 256, 2, 2]           262,144\n│    │    └─BatchNorm2d: 3-86            [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-87                   [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-88                 [10, 256, 2, 2]           589,824\n│    │    └─BatchNorm2d: 3-89            [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-90                   [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-91                 [10, 1024, 2, 2]          262,144\n│    │    └─BatchNorm2d: 3-92            [10, 1024, 2, 2]          2,048\n│    │    └─ReLU: 3-93                   [10, 1024, 2, 2]          --\n│    └─Bottleneck: 2-11                  [10, 1024, 2, 2]          --\n│    │    └─Conv2d: 3-94                 [10, 256, 2, 2]           262,144\n│    │    └─BatchNorm2d: 3-95            [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-96                   [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-97                 [10, 256, 2, 2]           589,824\n│    │    └─BatchNorm2d: 3-98            [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-99                   [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-100                [10, 1024, 2, 2]          262,144\n│    │    └─BatchNorm2d: 3-101           [10, 1024, 2, 2]          2,048\n│    │    └─ReLU: 3-102                  [10, 1024, 2, 2]          --\n│    └─Bottleneck: 2-12                  [10, 1024, 2, 2]          --\n│    │    └─Conv2d: 3-103                [10, 256, 2, 2]           262,144\n│    │    └─BatchNorm2d: 3-104           [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-105                  [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-106                [10, 256, 2, 2]           589,824\n│    │    └─BatchNorm2d: 3-107           [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-108                  [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-109                [10, 1024, 2, 2]          262,144\n│    │    └─BatchNorm2d: 3-110           [10, 1024, 2, 2]          2,048\n│    │    └─ReLU: 3-111                  [10, 1024, 2, 2]          --\n│    └─Bottleneck: 2-13                  [10, 1024, 2, 2]          --\n│    │    └─Conv2d: 3-112                [10, 256, 2, 2]           262,144\n│    │    └─BatchNorm2d: 3-113           [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-114                  [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-115                [10, 256, 2, 2]           589,824\n│    │    └─BatchNorm2d: 3-116           [10, 256, 2, 2]           512\n│    │    └─ReLU: 3-117                  [10, 256, 2, 2]           --\n│    │    └─Conv2d: 3-118                [10, 1024, 2, 2]          262,144\n│    │    └─BatchNorm2d: 3-119           [10, 1024, 2, 2]          2,048\n│    │    └─ReLU: 3-120                  [10, 1024, 2, 2]          --\n├─Sequential: 1-8                        [10, 2048, 1, 1]          --\n│    └─Conv2d: 2-14                      [10, 2048, 2, 2]          2,097,152\n│    └─BatchNorm2d: 2-15                 [10, 2048, 2, 2]          4,096\n│    └─ReLU: 2-16                        [10, 2048, 2, 2]          --\n│    └─AdaptiveAvgPool2d: 2-17           [10, 2048, 1, 1]          --\n├─Linear: 1-9                            [10, 11]                  22,539\n==========================================================================================\nTotal params: 10,667,083\nTrainable params: 10,667,083\nNon-trainable params: 0\nTotal mult-adds (M): 580.15\n==========================================================================================\nInput size (MB): 0.06\nForward/backward pass size (MB): 23.13\nParams size (MB): 42.67\nEstimated Total Size (MB): 65.86\n=========================================================================================="
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(MyResNet50, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # 保留resnet50的前几层\n",
    "        self.conv1 = resnet50.conv1\n",
    "        self.bn1 = resnet50.bn1\n",
    "        self.relu = resnet50.relu\n",
    "        self.maxpool = resnet50.maxpool\n",
    "        self.layer1 = resnet50.layer1\n",
    "        self.layer2 = resnet50.layer2\n",
    "        self.layer3 = resnet50.layer3\n",
    "\n",
    "        # 创建自定义的后3层\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 2048, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "from torchinfo import summary\n",
    "model = MyResNet50()\n",
    "summary(model, (10,3, 22, 22), device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyResNet34(torch.nn.Module):\n",
    "    def __init__(self, num_classes=11, pretrained=True, dropout_rate=0.5):\n",
    "        super(MyResNet34, self).__init__()\n",
    "        # 获取预训练的ResNet34模型并去除最后一层（全连接层）\n",
    "        resnet34 = models.resnet34(pretrained=pretrained)\n",
    "        resnet_without_fc = torch.nn.Sequential(*(list(resnet34.children())[:-1]))\n",
    "\n",
    "        self.model = resnet_without_fc\n",
    "\n",
    "        # 添加 Dropout 层\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 添加自定义线性层以输出11个类\n",
    "        self.fc = torch.nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        # 将输出展平以适用于全连接层\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # 通过 Dropout 层\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 添加最后的全连接层\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class MyResNet18(torch.nn.Module):\n",
    "    def __init__(self, num_classes=11, dropout_rate=0.5):\n",
    "        super(MyResNet18, self).__init__()\n",
    "\n",
    "        resnet18 = models.resnet18(pretrained=False)\n",
    "        original_layers = list(resnet18.children())\n",
    "\n",
    "        # 我们只保留其中两个基本块，每个基本块包含两个残差块，因此我们删除了 2 * 2 = 4 个残差块\n",
    "        reduced_resnet = nn.Sequential(\n",
    "            *original_layers[:4],\n",
    "            nn.Sequential(original_layers[4][0], original_layers[4][1]),\n",
    "            nn.Sequential(original_layers[5][0], original_layers[5][1]),\n",
    "            *original_layers[6:]\n",
    "        )\n",
    "\n",
    "        self.model = reduced_resnet[:-1]  # 除去原始 ResNet18 的全连接层\n",
    "\n",
    "        # 添加 Dropout 层\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 添加自定义线性层以输出11个类\n",
    "        self.fc = torch.nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        # 将输出展平以适用于全连接层\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # 通过 Dropout 层\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 添加最后的全连接层\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "C:\\Users\\admin\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 2]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchinfo\\torchinfo.py\u001B[0m in \u001B[0;36mforward_pass\u001B[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[0m\n\u001B[0;32m    287\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 288\u001B[1;33m                 \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    289\u001B[0m             \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1147\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1148\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1149\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_global_forward_hooks\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24992\\2909777739.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1147\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1148\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1149\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_global_forward_hooks\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    138\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 139\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    140\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1147\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1148\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1149\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_global_forward_hooks\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    134\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 135\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_input_dim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    136\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001B[0m in \u001B[0;36m_check_input_dim\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    406\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 407\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"expected 4D input (got {}D input)\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    408\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: expected 4D input (got 3D input)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24992\\2909777739.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMyResNet101\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m \u001B[0msummary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchinfo\\torchinfo.py\u001B[0m in \u001B[0;36msummary\u001B[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    217\u001B[0m     )\n\u001B[0;32m    218\u001B[0m     summary_list = forward_pass(\n\u001B[1;32m--> 219\u001B[1;33m         \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcache_forward_pass\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_mode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    220\u001B[0m     )\n\u001B[0;32m    221\u001B[0m     \u001B[0mformatting\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFormattingOptions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdepth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcol_width\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchinfo\\torchinfo.py\u001B[0m in \u001B[0;36mforward_pass\u001B[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[0m\n\u001B[0;32m    298\u001B[0m             \u001B[1;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    299\u001B[0m             \u001B[1;34mf\"Executed layers up to: {executed_layers}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 300\u001B[1;33m         ) from e\n\u001B[0m\u001B[0;32m    301\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    302\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhooks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 2]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyResNet101(torch.nn.Module):\n",
    "    def __init__(self, num_classes=11, pretrained=True):\n",
    "        super(MyResNet101, self).__init__()\n",
    "        # 获取预训练的ResNet101模型并删除最后一层（全连接层）\n",
    "        resnet101 = models.resnet101(pretrained=pretrained)\n",
    "        resnet_without_fc = torch.nn.Sequential(*(list(resnet101.children())[:-1]))\n",
    "\n",
    "        self.model = resnet_without_fc\n",
    "\n",
    "        # 添加自定义层\n",
    "        self.my_custom_layer1 = torch.nn.Linear(in_features=2048, out_features=1024)\n",
    "        self.my_custom_layer2 = torch.nn.ReLU(inplace=True)\n",
    "        self.my_custom_layer3 = torch.nn.Linear(in_features=1024, out_features=num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model(inputs)\n",
    "\n",
    "        # 将输出展平至适用于全连接层的维度\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # 执行自定义层\n",
    "        x = self.my_custom_layer1(x)\n",
    "        x = self.my_custom_layer2(x)\n",
    "        x = self.my_custom_layer3(x)\n",
    "\n",
    "        return x\n",
    "model = MyResNet101()\n",
    "summary(model, (3, 20, 20), device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience = 20, tol = 0.005): #惯例地定义我们所需要的一切变量/属性\\\n",
    "        #当连续patience次迭代时，这一轮迭代的损失与历史最低损失之间的差值小于阈值时\n",
    "        #就触发提前停止\n",
    "\n",
    "        self.patience = patience\n",
    "        self.tol = tol #tolerance，累积5次都低于tol才会触发停止\n",
    "        self.counter = 0 #计数，计算现在已经累积了counter次\n",
    "        self.lowest_loss = None\n",
    "        self.early_stop = False #True - 提前停止，False - 不要提前停止\n",
    "\n",
    "    def __call__(self,val_loss):\n",
    "        if self.lowest_loss == None: #这是第一轮迭代\n",
    "            self.lowest_loss = val_loss\n",
    "        elif self.lowest_loss - val_loss > self.tol:\n",
    "            self.lowest_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif self.lowest_loss - val_loss < self.tol:\n",
    "            self.counter += 1\n",
    "            print(\"\\t NOTICE: Early stopping counter {} of {}\".format(self.counter,self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                print('\\t NOTICE: Early Stopping Actived')\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "        #这一轮迭代的损失与历史最低损失之间的差 - 阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IterOnce(net,criterion,opt,x,y):\n",
    "    \"\"\"\n",
    "    对模型进行一次迭代的函数\n",
    "\n",
    "    net: 实例化后的架构\n",
    "    criterion: 损失函数\n",
    "    opt: 优化算法\n",
    "    x: 这一个batch中所有的样本\n",
    "    y: 这一个batch中所有样本的真实标签\n",
    "    \"\"\"\n",
    "    sigma = net.forward(x)\n",
    "    loss = criterion(sigma,y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad(set_to_none=True) #比起设置梯度为0，让梯度为None会更节约内存\n",
    "    yhat = torch.max(sigma,1)[1]\n",
    "    correct = torch.sum(yhat == y)\n",
    "    return correct,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestOnce(net,criterion,x,y):\n",
    "    \"\"\"\n",
    "    对一组数据进行测试并输出测试结果的函数\n",
    "\n",
    "    net: 经过训练后的架构\n",
    "    criterion：损失函数\n",
    "    x：要测试的数据的所有样本\n",
    "    y：要测试的数据的真实标签\n",
    "    \"\"\"\n",
    "    #对测试，一定要阻止计算图追踪\n",
    "    #这样可以节省很多内存，加速运算\n",
    "    with torch.no_grad():\n",
    "        sigma = net.forward(x)\n",
    "        loss = criterion(sigma,y)\n",
    "        yhat = torch.max(sigma,1)[1]\n",
    "        correct = torch.sum(yhat == y)\n",
    "    return correct,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_test(net, batchdata, testdata, criterion, opt, epochs, tol, modelname, PATH):\n",
    "\n",
    "    SamplePerEpoch = batchdata.dataset.__len__()\n",
    "    allsamples = SamplePerEpoch * epochs\n",
    "    trainedsamples = 0\n",
    "    trainlosslist = []\n",
    "    testlosslist = []\n",
    "    early_stopping = EarlyStopping(tol = tol)\n",
    "    highestacc = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        loss_train = 0\n",
    "        correct_train = 0\n",
    "        for batch_idx, (x, y) in enumerate(batchdata):\n",
    "            x = x.to(device, non_blocking = True)\n",
    "            y = y.to(device, non_blocking = True).view(x.shape[0])\n",
    "            correct, loss = IterOnce(net, criterion, opt, x, y)\n",
    "            trainedsamples += x.shape[0]\n",
    "            loss_train += loss\n",
    "            correct_train += correct\n",
    "\n",
    "            if (batch_idx + 1) % 81 == 0:\n",
    "                print('Epoch{}:[{}/{}({:.0f}%)]'.format( epoch\n",
    "                                                         ,trainedsamples\n",
    "                                                         ,allsamples\n",
    "                                                         ,100*trainedsamples/allsamples))\n",
    "\n",
    "        TrainAccThisEpoch = float(correct_train*100)/SamplePerEpoch\n",
    "        TrainLossThisEpoch = float(loss_train*100)/SamplePerEpoch\n",
    "        trainlosslist.append(TrainLossThisEpoch)\n",
    "\n",
    "        del x, y, correct, loss, correct_train, loss_train\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        net.eval()\n",
    "        loss_test = 0\n",
    "        correct_test = 0\n",
    "        TestSample = testdata.dataset.__len__()\n",
    "\n",
    "        for x, y in testdata:\n",
    "            with torch.no_grad():\n",
    "                x = x.to(device, non_blocking = True)\n",
    "                y = y.to(device, non_blocking = True).view(x.shape[0])\n",
    "                correct, loss = TestOnce(net, criterion, x, y)\n",
    "                loss_test += loss\n",
    "                correct_test += correct\n",
    "\n",
    "        TestAccThisEpoch = float(correct_test*100)/TestSample\n",
    "        TestLossThisEpoch = float(loss_test*100)/TestSample\n",
    "        testlosslist.append(TestLossThisEpoch)\n",
    "\n",
    "        print(\"\\t Train Loss:{:.6f}, Test Loss:{:.6f}, Train Acc:{:.3f}%, Test Acc:{:.3f}%\".format(TrainLossThisEpoch ,TestLossThisEpoch, TrainAccThisEpoch, TestAccThisEpoch))\n",
    "\n",
    "        if highestacc == None:\n",
    "            highestacc = TestAccThisEpoch\n",
    "        if highestacc < TestAccThisEpoch:\n",
    "            highestacc = TestAccThisEpoch\n",
    "            torch.save(net.state_dict(), os.path.join(PATH, modelname+\".pt\"))\n",
    "            print(\"\\t Weight Saved\")\n",
    "\n",
    "\n",
    "        #EarlyStopping\n",
    "        early_stop = early_stopping(TestLossThisEpoch)\n",
    "        if early_stop == True:\n",
    "            break\n",
    "\n",
    "    print(\"Mission Complete\")\n",
    "    return  trainlosslist,testlosslist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_procedure(net,epochs,bs,modelname, PATH, lr=0.001,alpha=0.99,gamma=0,wd=0,tol=10**(-5)):\n",
    "\n",
    "    torch.manual_seed(1412)\n",
    "    torch.cuda.manual_seed(1412)\n",
    "    torch.cuda.manual_seed_all(1412)\n",
    "    #分割数据\n",
    "    batchdata = DataLoader(train,batch_size=bs,shuffle=True\n",
    "                           ,drop_last=False, pin_memory=True) #线程 - 调度计算资源的最小单位\n",
    "    testdata = DataLoader(test,batch_size=bs,shuffle=False\n",
    "                          ,drop_last=False, pin_memory=True)\n",
    "\n",
    "    #损失函数，优化算法\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\") #进行损失函数计算时，最后输出结果的计算模式\n",
    "    # opt = optim.RMSprop(net.parameters(),lr=lr\n",
    "    #                     ,alpha=alpha,momentum=gamma,weight_decay=wd)\n",
    "    opt = optim.SGD(net.parameters(),lr=lr\n",
    "                        ,momentum=0.9,weight_decay=1e-2)\n",
    "    #训练与测试\n",
    "    trainloss, testloss = fit_test(net,batchdata,testdata,criterion,opt,epochs,tol,modelname,PATH)\n",
    "\n",
    "    return trainloss, testloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotloss(trainloss, testloss):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(trainloss, color=\"red\", label=\"Trainloss\")\n",
    "    plt.plot(testloss, color=\"orange\", label=\"Testloss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "C:\\Users\\admin\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1:[6480/4575000(0%)]\n",
      "\t Train Loss:311.314727, Test Loss:335.298116, Train Acc:13.410%, Test Acc:7.072%\n",
      "Epoch2:[15630/4575000(0%)]\n",
      "\t Train Loss:233.692104, Test Loss:285.789508, Train Acc:16.404%, Test Acc:10.174%\n",
      "\t Weight Saved\n",
      "Epoch3:[24780/4575000(1%)]\n",
      "\t Train Loss:222.999686, Test Loss:269.974256, Train Acc:19.005%, Test Acc:10.670%\n",
      "\t Weight Saved\n",
      "Epoch4:[33930/4575000(1%)]\n",
      "\t Train Loss:215.692350, Test Loss:387.466579, Train Acc:22.689%, Test Acc:10.670%\n",
      "\t NOTICE: Early stopping counter 1 of 20\n",
      "Epoch5:[43080/4575000(1%)]\n",
      "\t Train Loss:209.508087, Test Loss:277.828784, Train Acc:25.749%, Test Acc:13.151%\n",
      "\t Weight Saved\n",
      "\t NOTICE: Early stopping counter 2 of 20\n",
      "Epoch6:[52230/4575000(1%)]\n",
      "\t Train Loss:204.818579, Test Loss:282.745580, Train Acc:26.940%, Test Acc:11.538%\n",
      "\t NOTICE: Early stopping counter 3 of 20\n",
      "Epoch7:[61380/4575000(1%)]\n",
      "\t Train Loss:202.305792, Test Loss:263.849566, Train Acc:27.858%, Test Acc:11.414%\n",
      "Epoch8:[70530/4575000(2%)]\n",
      "\t Train Loss:201.367568, Test Loss:266.339427, Train Acc:29.213%, Test Acc:14.020%\n",
      "\t Weight Saved\n",
      "\t NOTICE: Early stopping counter 1 of 20\n",
      "Epoch9:[79680/4575000(2%)]\n",
      "\t Train Loss:198.416940, Test Loss:275.960201, Train Acc:29.727%, Test Acc:11.538%\n",
      "\t NOTICE: Early stopping counter 2 of 20\n",
      "Epoch10:[88830/4575000(2%)]\n",
      "\t Train Loss:196.113415, Test Loss:283.468653, Train Acc:30.809%, Test Acc:14.020%\n",
      "\t NOTICE: Early stopping counter 3 of 20\n",
      "Epoch11:[97980/4575000(2%)]\n",
      "\t Train Loss:193.473852, Test Loss:284.870154, Train Acc:32.306%, Test Acc:11.911%\n",
      "\t NOTICE: Early stopping counter 4 of 20\n",
      "Epoch12:[107130/4575000(2%)]\n",
      "\t Train Loss:193.890478, Test Loss:266.869785, Train Acc:32.459%, Test Acc:11.414%\n",
      "\t NOTICE: Early stopping counter 5 of 20\n",
      "Epoch13:[116280/4575000(3%)]\n",
      "\t Train Loss:191.630779, Test Loss:272.836635, Train Acc:33.148%, Test Acc:13.648%\n",
      "\t NOTICE: Early stopping counter 6 of 20\n",
      "Epoch14:[125430/4575000(3%)]\n",
      "\t Train Loss:188.539727, Test Loss:269.339853, Train Acc:34.809%, Test Acc:12.779%\n",
      "\t NOTICE: Early stopping counter 7 of 20\n",
      "Epoch15:[134580/4575000(3%)]\n",
      "\t Train Loss:186.935150, Test Loss:280.669975, Train Acc:34.831%, Test Acc:12.035%\n",
      "\t NOTICE: Early stopping counter 8 of 20\n",
      "Epoch16:[143730/4575000(3%)]\n",
      "\t Train Loss:185.859262, Test Loss:261.960162, Train Acc:35.902%, Test Acc:14.640%\n",
      "\t Weight Saved\n",
      "Epoch17:[152880/4575000(3%)]\n",
      "\t Train Loss:184.393169, Test Loss:269.933022, Train Acc:36.842%, Test Acc:14.144%\n",
      "\t NOTICE: Early stopping counter 1 of 20\n",
      "Epoch18:[162030/4575000(4%)]\n",
      "\t Train Loss:182.587801, Test Loss:272.285941, Train Acc:36.153%, Test Acc:11.538%\n",
      "\t NOTICE: Early stopping counter 2 of 20\n",
      "Epoch19:[171180/4575000(4%)]\n",
      "\t Train Loss:182.316571, Test Loss:269.499574, Train Acc:36.896%, Test Acc:12.035%\n",
      "\t NOTICE: Early stopping counter 3 of 20\n",
      "Epoch20:[180330/4575000(4%)]\n",
      "\t Train Loss:180.387760, Test Loss:274.973015, Train Acc:37.694%, Test Acc:12.407%\n",
      "\t NOTICE: Early stopping counter 4 of 20\n",
      "Epoch21:[189480/4575000(4%)]\n",
      "\t Train Loss:178.719959, Test Loss:277.251570, Train Acc:39.148%, Test Acc:10.174%\n",
      "\t NOTICE: Early stopping counter 5 of 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24992\\926040293.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m                                          \u001B[1;33m,\u001B[0m\u001B[0mmodelname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"model_seletion_resnet50\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m                                          \u001B[1;33m,\u001B[0m\u001B[0mPATH\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPATH\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m                                          ,tol = 10**(-3))\n\u001B[0m\u001B[0;32m     28\u001B[0m     \u001B[0mavgtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[0mplotloss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrainloss\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtestloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24992\\3872947967.py\u001B[0m in \u001B[0;36mfull_procedure\u001B[1;34m(net, epochs, bs, modelname, PATH, lr, alpha, gamma, wd, tol)\u001B[0m\n\u001B[0;32m     17\u001B[0m                         ,momentum=0.9,weight_decay=1e-2)\n\u001B[0;32m     18\u001B[0m     \u001B[1;31m#训练与测试\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m     \u001B[0mtrainloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtestloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfit_test\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mbatchdata\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtestdata\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mopt\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtol\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmodelname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mPATH\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtrainloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtestloss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24992\\1240690325.py\u001B[0m in \u001B[0;36mfit_test\u001B[1;34m(net, batchdata, testdata, criterion, opt, epochs, tol, modelname, PATH)\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mloss_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m         \u001B[0mcorrect_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatchdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m             \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m             \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    679\u001B[0m                 \u001B[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    680\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 681\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    682\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    683\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    719\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    720\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 721\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    722\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    723\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory_device\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    230\u001B[0m         \u001B[0msample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    231\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 232\u001B[1;33m             \u001B[0msample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    233\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_transform\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    234\u001B[0m             \u001B[0mtarget\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 94\u001B[1;33m             \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     95\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, tensor)\u001B[0m\n\u001B[0;32m    267\u001B[0m             \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mNormalized\u001B[0m \u001B[0mTensor\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    268\u001B[0m         \"\"\"\n\u001B[1;32m--> 269\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormalize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    270\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    271\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001B[0m in \u001B[0;36mnormalize\u001B[1;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[0;32m    358\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 360\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mF_t\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnormalize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    361\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\python3.7\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py\u001B[0m in \u001B[0;36mnormalize\u001B[1;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[0;32m    946\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    947\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 948\u001B[1;33m         \u001B[0mtensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    949\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    950\u001B[0m     \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#建立目录用于存储模型选择结果\n",
    "PATH = r\"D:\\Desktop\"\n",
    "\n",
    "#使用函数full_procedure中的默认参数，在模型选择时加入时间计算\n",
    "#基于现有显存，batch_size被设置为256，对于CPU而言最好从64开始设置\n",
    "#MyResNet\n",
    "avgtime = [] #用来存放每次循环之后获得的训练时间\n",
    "for i in range(1): #进行5次训练\n",
    "    #设置随机数种子\n",
    "    torch.manual_seed(1412)\n",
    "    torch.cuda.manual_seed(1412)\n",
    "    torch.cuda.manual_seed_all(1412)\n",
    "    #实例化\n",
    "    # resnet18_ = m.resnet18()\n",
    "    # resnet50_ = m.resnet50()\n",
    "    # net = MyResNet50().to(device,non_blocking=True)\n",
    "\n",
    "    # 实例化自定义模型\n",
    "    net = MyResNet18(num_classes=11).to(device,non_blocking=True)\n",
    "    # VGG_ = m.vgg16_bn()\n",
    "    # net = MyVGG().to(device,non_blocking=True)\n",
    "    #训练\n",
    "    start = time() #计算训练时间\n",
    "    trainloss, testloss = full_procedure(net,epochs=500, bs=80\n",
    "                                         ,modelname=\"model_seletion_resnet50\"\n",
    "                                         ,PATH = PATH\n",
    "                                         ,tol = 10**(-3))\n",
    "    avgtime.append(time()-start)\n",
    "    plotloss(trainloss,testloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# del net\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(np.mean(avgtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
